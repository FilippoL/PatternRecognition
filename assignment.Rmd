---
title: "R Notebook"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---


```{r warning=FALSE}
# install.packages("OpenImageR") 
# install.packages("dplyr")
# install.packages("iterators")
# install.packages("nnet")
# install.packages("reshape2")
# install.packages("ggplot2")
# install.packages("caret")
# install.packages("glmnet")
# install.packages("e1071")
# install.packages("factoextra")

```


```{r warning=FALSE}
library(OpenImageR) 
library(dplyr)
library(iterators)
library(nnet)
library(reshape2)
library(ggplot2)
library(caret)
library(glmnet)
library(e1071)
library(factoextra)
```
```{r}
mnist.dat <- read.csv("mnist.csv")
pixels_data = mnist.dat[,-1]
scaled_pixels_data = scale(pixels_data)
scaled_pixels_data[is.na(scaled_pixels_data)] = 0 #Replacing NaN with 0
mnist.dat$label = as.factor(mnist.dat$label)
```
```{r}
show_image_from_data = function(data, row) {
  sample_image = t(matrix(as.numeric(data[row,-1]),nrow=28,ncol=28,byrow=T)[c(28:1),,drop = FALSE])
  return (image(sample_image, axes = FALSE, col = grey(seq(0, 1, length = 256)), asp=1))
}
show_image_data = function(actual_digit) {
  return (t(matrix(as.numeric(actual_digit),nrow=28,ncol=28,byrow=T))[,c(28:1),drop = FALSE])
}
show_image_from_data(mnist.dat, 1)  

```
```{r}
mnist.summary = data.frame(t(sapply( pixels_data , function(x) cbind(mean = mean(x) ,
                                  sd = sd(x) ,
                                  median = median(x) ,
                                  minimum = min(x) ,
                                  maximum = max(x) ,
                                  s.size = length(x)))))
colnames(mnist.summary) = c("mean","sd","median","minimum","maximum","size")
```
```{r}
head(mnist.summary[mnist.summary$sd==0,])
```
```{r}
indices_of_useless_features = which(mnist.summary$sd == 0)
actual_digit = mnist.dat[90,-1]
actual_digit[,indices_of_useless_features] = 125
digit_example = show_image_data(actual_digit)
image(digit_example, axes = FALSE, col = grey(seq(0, 1, length = 256)), asp=1)
```
```{r}
indices_of_useless_features = which(mnist.summary$sd < 1)
actual_digit = mnist.dat[400,-1]
actual_digit[,indices_of_useless_features] = 125
digit_example = show_image_data(actual_digit)
image(digit_example, axes = FALSE, col = grey(seq(0, 1, length = 256)), asp=1)
```

```{r}
# Plot count of each digit's pixel in barplot and then display it in a table
label_distribution = table(mnist.dat[,1]) 
barplot(label_distribution, col=rainbow(10, 0.5), main="Digits in dataset")
# as.data.frame(mnist.dat) %>% group_by(label) %>% summarise(count = n())
```
```{r}
expected_accuracy_on_majority_label_classification = label_distribution[2] / sum(label_distribution)
sprintf("Accuracy if 1 was predicted for all labels: %.2f %%", expected_accuracy_on_majority_label_classification * 100)
```

Threshold decision

[useful link](https://stackoverflow.com/questions/25360248/arrange-multiple-32-png-files-in-a-grid/51865036#51865036)

Even after removing some data we can still see the picture as follows:

```{r}
actual_digit = mnist.dat[450,-1]
indices_of_useful_features = which(actual_digit < 248)
actual_digit[,indices_of_useful_features] = 0
digit_example = show_image_data(actual_digit)
image(digit_example, axes = FALSE, col = grey(seq(0, 1, length = 256)), asp=1)
```

```{r}
hist(as.numeric(actual_digit), main="Frequency of pixels values", xlab="Pixel Value", ylab="Frequency")

```

```{r}
hist.data = hist(as.matrix(mnist.dat[,-1]), breaks=32, main="Frequency of pixels values", xlab="Pixel Value", ylab="Frequency")
hist.data$counts = log10(hist.data$counts)
plot(hist.data, main="Frequency of scaled pixels values", xlab="Pixel Value", ylab="Frequency")
axis(side=1, at=seq(0, 256, 8))
```

```{r}
# indices_of_useful_features = 
# function(x) x[which(x < 248)] = 0
cleaned_images = t(apply(pixels_data, MARGIN = 1, FUN=function(x) replace(x, which(x < 5), 0)))
image_density = apply(cleaned_images, MARGIN=1, FUN=function(x) sum(x))
image_density_per_pixel = apply(data.frame(image_density), MARGIN=1, FUN=function(x) sum(x)/784) # Calculate density
data = cbind.data.frame(density=image_density, 
                        density_per_pixel=scale(image_density_per_pixel), 
                        scaled_density = scale(image_density), 
                        label=mnist.dat$label)


data2 = cbind.data.frame(log_dens=scale(log(image_density_per_pixel)), 
                        sqrt_dens=scale(sqrt(image_density_per_pixel)), 
                        sin_dens=scale(sin(image_density_per_pixel)), 
                        tanh_dens=tanh(scale(image_density_per_pixel)),  #TODO: Explain why scale after
                        scaled_density = scale(image_density),
                        squared_dens=scale(image_density^2),
                        label=mnist.dat$label)

head(data)
head(data2)

```

```{r}
summary_per_digit = rbind.data.frame(tapply(data$density_per_pixel, data$label,  function(x) cbind(mean = mean(x) ,
                                  sd = sd(x) ,
                                  median = median(x) ,
                                  minimum = min(x) ,
                                  maximum = max(x),
                                  s.size = length(x))
                           ))
rownames(summary_per_digit) = c("mean","sd","median","minimum","maximum","size") 
#TODO: Plot all distributions
data.frame(t(summary_per_digit))
```

```{r}
cormat = dist(t(summary_per_digit[c("mean","sd"),]))
data.frame(as.matrix(cormat))
```
```{r}
cormat = dist(t(summary_per_digit[c("mean","sd","median","minimum","maximum"),]))
melted_cormat <- melt(as.matrix(cormat))
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + geom_tile()
```
```{r}
density_model = multinom(label ~ density_per_pixel, data, maxit=1000)
density_model

```
```{r}
density_model2 = multinom(label ~ scaled_density + log_dens, data2, maxit=1000)
density_model2

predicted = predict(density_model2, data2)
confmat = table(data2$label, predicted)
confmat
sum(diag(confmat))/sum(confmat)

```

```{r}
predicted = predict(density_model, data)
confmat = table(data$label, predicted)
confmat
```
```{r}
sum(diag(confmat))/sum(confmat)
```

```{r}
#pixels_data = mnist.dat[,-1]
# scaled_pixels_data <- mnist.dat[,2:784]/255
# down_sampled_images = pixels_data %>% as.matrix(ncol=28, nrow=28) %>% down_sample_image(factor=2)
# scaled_pixels_data = scale(pixels_data)
# scaled_pixels_data[is.na(scaled_pixels_data)] = 0 #Replacing NaN with 0
data_for_ml = cbind.data.frame(label=mnist.dat$label, scaled_pixels_data)
training_data = sample_n(data_for_ml, 5000)
validation_data = setdiff(data_for_ml, training_data) #TODO: check what is going on
```

```{r}
trained_model = multinom(label ~., family = "multinomial", data = training_data, MaxNWts =10000000, maxit=50)
```

```{r}
predicted = predict(trained_model, training_data[,-1])
confmat = table(training_data$label, predicted)
confmat
sum(diag(confmat))/sum(confmat)
```

```{r}
predicted = predict(trained_model, validation_data[,-1])
confmat = table(validation_data$label, predicted)
confmat
sum(diag(confmat))/sum(confmat)
```
```{r}
trained_model = svm(label ~., data = training_data)
predicted = predict(trained_model, validation_data[,-1])
confmat = table(validation_data$label, predicted)
confmat
sum(diag(confmat))/sum(confmat)
```

```{r}
trained_model = nnet(label ~., data = training_data, size=5,MaxNWts=5000)
predicted = predict(trained_model, validation_data[,-1], type="class")
confmat = table(validation_data$label, predicted)
confmat
sum(diag(confmat))/sum(confmat)
```
```{r}
tuned_model_mulinom <- cv.glmnet(as.matrix(training_data[,-c(1)]), training_data$label,family="multinomial", type.measure="class")
plot(tuned_model_mulinom)
predicted = predict(tuned_model_mulinom, as.matrix(validation_data[,-1]), type="class")
confmat = table(validation_data$label, predicted)
confmat
sum(diag(confmat))/sum(confmat)
```

```{r}

```


```{r}
tuned_results_svm = tune.svm(label ~., data = training_data, cost=1:10, gamma = c(0.1, 0.01, 0.001, 0.0001, 0.00001))
tuned_results_svm$performances
```
```{r}
tuned_results_nnet = tune.nnet(label ~., data = training_data, size=1:10, decay=c(0.1, 0.01, 0.001, 0.0001, 0.00001))
tuned_results_nnet$performances
```
```{r}
tuned_model_svm = svm(label ~., data = training_data) # TODO: Add cost, gamma
predicted = predict(tuned_model_svm, validation_data[,-1])
confmat = table(validation_data$label, predicted)
confmat
sum(diag(confmat))/sum(confmat)
```
```{r}
tuned_model_nnet = nnet(label ~., data = training_data, size=5, MaxNWts=5000) # TODO: Add size, decay
predicted = predict(tuned_model_nnet, validation_data[,-1], type="class")
confmat = table(validation_data$label, predicted)
confmat
sum(diag(confmat))/sum(confmat)
```

```{r}
pca_model = prcomp(scaled_pixels_data[,-indices_of_useless_features])
```
```{r}
principal_components = as.matrix(pca_model$x)
```

```{r}
data3 = cbind.data.frame(scaled_density = scale(image_density),
                         first_pca = scale(principal_components[,1]),
                        label=mnist.dat$label)
head(data3)
```


```{r}
density_model3 = multinom(label ~ scaled_density + first_pca, data3, maxit=1000)
density_model3

predicted = predict(density_model3, data3)
confmat = table(data2$label, predicted)
confmat
sum(diag(confmat))/sum(confmat)
```

